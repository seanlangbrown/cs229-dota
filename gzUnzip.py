# generated by claude sonnet 3.5
import boto3
import gzip
import io
import logging
import sys
from typing import Generator, Any
from botocore.exceptions import ClientError

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class S3StreamProcessor:
    def __init__(self, source_bucket: str, source_key: str, 
                 dest_bucket: str, dest_key: str,
                 part_size_mb: int = 100):
        """
        Initialize the processor.
        
        Args:
            source_bucket: Source S3 bucket name
            source_key: Source file key (gzipped file)
            dest_bucket: Destination S3 bucket name
            dest_key: Destination file key (JSON file)
            part_size_mb: Size of each upload part in MB
        """
        self.source_bucket = source_bucket
        self.source_key = source_key
        self.dest_bucket = dest_bucket
        self.dest_key = dest_key
        self.part_size = part_size_mb * 1024 * 1024  # Convert MB to bytes
        self.s3_client = boto3.client('s3')
        
    def stream_gzip_from_s3(self) -> Generator[str, None, None]:
        """Stream and decompress gzipped content from S3."""
        try:
            # Get the gzipped object from S3
            response = self.s3_client.get_object(
                Bucket=self.source_bucket,
                Key=self.source_key
            )
            
            # Create a gzip reader around the StreamingBody
            with gzip.GzipFile(fileobj=response['Body'], mode='rb') as gz:
                # Read and yield lines
                for line in io.TextIOWrapper(gz, encoding='utf-8'):
                    yield line.strip()
                    
        except ClientError as e:
            logger.error(f"Error reading from S3: {str(e)}")
            raise
            
    def stream_to_s3(self, data_generator: Generator[str, None, None]) -> None:
        """Stream data to S3 using multipart upload."""
        try:
            # Initialize multipart upload
            mpu = self.s3_client.create_multipart_upload(
                Bucket=self.dest_bucket,
                Key=self.dest_key
            )
            
            logger.info(f"Started multipart upload to s3://{self.dest_bucket}/{self.dest_key}")
            
            part_number = 1
            current_part = []
            current_size = 0
            parts = []
            bytes_processed = 0
            
            try:
                for line in data_generator:
                    # Add newline to maintain JSON line format
                    line_bytes = (line + '\n').encode('utf-8')
                    line_size = len(line_bytes)
                    
                    # If adding this line would exceed part size, upload current part
                    if current_size + line_size > self.part_size:
                        # Upload current part
                        part_data = ''.join(current_part).encode('utf-8')
                        response = self.s3_client.upload_part(
                            Bucket=self.dest_bucket,
                            Key=self.dest_key,
                            PartNumber=part_number,
                            UploadId=mpu['UploadId'],
                            Body=part_data
                        )
                        
                        parts.append({
                            'PartNumber': part_number,
                            'ETag': response['ETag']
                        })
                        
                        bytes_processed += current_size
                        logger.info(f"Uploaded part {part_number}, "
                                  f"processed {bytes_processed / 1024 / 1024:.2f} MB")
                        
                        part_number += 1
                        current_part = [line]
                        current_size = line_size
                    else:
                        current_part.append(line)
                        current_size += line_size
                
                # Upload final part if any data remains
                if current_part:
                    part_data = ''.join(current_part).encode('utf-8')
                    response = self.s3_client.upload_part(
                        Bucket=self.dest_bucket,
                        Key=self.dest_key,
                        PartNumber=part_number,
                        UploadId=mpu['UploadId'],
                        Body=part_data
                    )
                    
                    parts.append({
                        'PartNumber': part_number,
                        'ETag': response['ETag']
                    })
                    
                    bytes_processed += current_size
                    logger.info(f"Uploaded final part {part_number}, "
                              f"total processed {bytes_processed / 1024 / 1024:.2f} MB")
                
                # Complete multipart upload
                self.s3_client.complete_multipart_upload(
                    Bucket=self.dest_bucket,
                    Key=self.dest_key,
                    UploadId=mpu['UploadId'],
                    MultipartUpload={'Parts': parts}
                )
                logger.info("Multipart upload completed successfully")
                
            except Exception as e:
                logger.error(f"Error during upload: {str(e)}")
                # Abort multipart upload
                self.s3_client.abort_multipart_upload(
                    Bucket=self.dest_bucket,
                    Key=self.dest_key,
                    UploadId=mpu['UploadId']
                )
                raise
                
        except Exception as e:
            logger.error(f"Error initializing upload: {str(e)}")
            raise
            
    def process(self) -> None:
        """Execute the full processing pipeline."""
        try:
            logger.info(f"Starting processing of s3://{self.source_bucket}/{self.source_key}")
            
            # Stream from source and upload to destination
            data_generator = self.stream_gzip_from_s3()
            self.stream_to_s3(data_generator)
            
            logger.info("Processing completed successfully")
            
        except Exception as e:
            logger.error(f"Processing failed: {str(e)}")
            raise

def main():
    # Example usage
    processor = S3StreamProcessor(
        source_bucket='cs229-dota',
        source_key='downloads/yasp-dump-2015-12-18.json.gz',
        dest_bucket='cs229-dota',
        dest_key='data/yasp-dump-2015-12-18.json',
        part_size_mb=100
    )
    
    processor.process()

if __name__ == "__main__":
    main()